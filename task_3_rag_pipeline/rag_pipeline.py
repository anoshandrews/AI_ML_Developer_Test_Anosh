import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline

EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
QA_MODEL = "distilbert-base-uncased-distilled-squad" 
# DistilBERT is proven good for extractive QA tasks: given a context, extract the answer span.
# Example:
# Context: "Albert Einstein was a physicist."
# Question: "Who was Albert Einstein?"
# Answer: "a physicist"

TOP_K = 3 # Top_k documents for RAG

import glob
import os

# Loading the documents
corpus_text = ""
for file_path in glob.glob("docs/*.txt"):
    print(f"âœ… Loading: {file_path}")
    with open(file_path, "r") as f:
        corpus_text += f.read() + "\n"

if corpus_text.strip() == "":
    raise ValueError("No documents found in docs/*.txt. Add some .txt files there!")

print("âœ… All documents loaded.")

# Split into chunks
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100
)
chunks = splitter.split_text(corpus_text)

print(f"ðŸŸ¢ Loaded {len(chunks)} document chunks.")

# ---------------------------
# Compute Embeddings
# ---------------------------

embedder = SentenceTransformer(EMBEDDING_MODEL)
embeddings = embedder.encode(chunks, show_progress_bar=True)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

print("ðŸŸ¢ FAISS index created.")

# ---------------------------
# Build QA Pipeline
# ---------------------------

qa_pipeline = pipeline("question-answering", model=QA_MODEL)

# ---------------------------
# RAG Pipeline
# ---------------------------

def rag_answer(question, top_k=TOP_K):
    """
    Generate an answer to a question using a Retrieval-Augmented Generation (RAG) approach.

    This function performs the following steps:
    1. Encodes the input question into an embedding.
    2. Searches for the top-k most similar document chunks based on vector similarity.
    3. Concatenates the retrieved chunks into a single context string.
    4. Uses an extractive QA pipeline to generate an answer from the context.

    Parameters
    ----------
    question : str
        The user question for which an answer is required.

    top_k : int, optional
        The number of top similar document chunks to retrieve for context. 
        Default is TOP_K.

    Returns
    -------
    str
        The extracted answer generated by the QA model based on the retrieved context.
    """
    # Encode question
    q_embedding = embedder.encode([question])
    
    # Search similar chunks
    distances, indices = index.search(np.array(q_embedding), top_k)
    relevant_chunks = [chunks[i] for i in indices[0]]
    
    # Concatenate context
    context = "\n".join(relevant_chunks)
    
    # Run extractive QA
    result = qa_pipeline(
        question=question,
        context=context
    )
    
    return result["answer"]

# ---------------------------
# Run Test Questions
# ---------------------------

questions = [
    "What is diabetes?",
    "How can AI help in diabetes treatment?",
    "What are symptoms of diabetes?"
]

for q in questions:
    print("\n=== QUESTION ===")
    print(q)
    answer = rag_answer(q)
    print("â†’ ANSWER:", answer)